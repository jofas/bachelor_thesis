\documentclass[twoside,11pt]{article}
\usepackage{luacode}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[page]{appendix}
\usepackage{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{tabu}
\usepackage{longtable}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}
\usepackage{float}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def\ds{\Lbag z_1,\dots,z_n \Rbag}
\def\X{\textbf{X}}
\def\Y{\textbf{Y}}
\def\Z{\textbf{Z}}
\def\S{\textbf{S}_{\Z}}

\begin{document}
% titlepage {{{
\begin{titlepage}
  \begin{flushleft}
	\vspace*{-1cm}
	\includegraphics[scale=0.05]{TH.png}\\
	\vspace*{1cm}
\end{flushleft}
\begin{center}
\begin{LARGE}
\textbf{%
	Approximating the optimal threshold for an abstaining
  classifier based on a reward function with regression
}
\end{LARGE}
~\\
~\\
~\\
\textit{{\LARGE B}ACHELOR {\LARGE T}HESIS}
~\\
~\\
~\\
\begin{Large}
\begin{tabu} to \textwidth {Xr}
Jonas Fassbender
&\href{mailto:jonas@fassbender.dev}{jonas@fassbender.dev}\\
&11117674
\end{tabu}
\end{Large}
~\\
~\\
~\\
\begin{large}
In the course of studies

\textit{{\Large C}OMPUTER {\Large S}CIENCE}
~\\
~\\
~\\
For the degree of

\textit{{\Large B}ACHELOR OF {\Large S}CIENCE}
~\\
~\\
~\\
Technical University of Cologne

Faculty of Computer Science and Engineering
~\\
~\\
~\\
\begin{tabular}{rl}
  First supervisor: &Prof. Dr. Heinrich Klocke\\
                    &Technical University of Cologne\\
  &\\
  Second supervisor: &Prof. Dr. Fotios Giannakopoulos\\
                     &Technical University of Cologne
\end{tabular}
~\\
~\\
~\\
Overath, July 2019
\end{large}
\end{center}
\end{titlepage}
% }}}

\begin{abstract}%
This thesis discusses a reward setting applied to
determining abstaining classifiers.
Abstaining classifiers are part of reliable machine
learning, which is applied in real world application
domains where wrong predictions are very expensive,
for example in medicine or finance.
While a cost sensitive setting for determining abstaining
classifiers is well known and researched, the reward
setting this thesis is based on, is not.

This thesis describes the reward setting it is based on and
proposes a general and flexible approach to determining
abstaining classifiers from reward.
Similarities to reinforcement learning---from which the
idea of reward came---are shown.

The proposed method is unstable and can produce non-optimal
abstaining classifiers---a direct consequence of its
generality and the small scope on which was experimented.
The method is intended to be the underlying framework
for more specialized and sophisticated methods, which are
outlined in this thesis.

While being unstable, empirical tests on real world data
sets show that the proposed method can produce approximate
optimal abstaining classifiers.
\end{abstract}

\begin{keywords}
Abstaining classifiers, classification with rejection,
reliable machine learning
\end{keywords}

% Introduction {{{
\section{Introduction}
\label{sec:intro}

An abstaining classifier
\citep[see e.g.][]{vanderlooy_et_al_2009}---also called a
classifier with reject option
\citep[see e.g.][]{fisher_et_al_2016}---is a kind
of confidence predictor.
It can refuse from making a prediction if its confidence in
the prediction is not high enough.
High enough, in this context, means that the confidence is
greater than a certain---hopefully optimal---threshold.
Optimality is dependent on a performance metric set
beforehand.

This thesis introduces a new kind of method for
approximating the optimal threshold based on a reward
function---better known from reinforcement learning than
from the supervised learning setting
\citep[see e.g.][Chapter 1]{sutton_et_al_2018}.
The method treats the reward function as unknown, making it
a very general approach and giving quite the amount of
freedom in designing the reward function.

In supervised learning the concept that is closest to a
reward function is a cost function and many abstract types
of cost in supervised learning are known
\citep[see][]{turney_2000}.

Probably today's most used methods for obtaining the
optimal threshold for reducing the expected cost of an
abstaining classifier are based on the receiver operating
characteristic (ROC) rule
\citep[see][]{tortella_2000,pietraszek_2005,
  vanderlooy_et_al_2009, guan_et_al_2018}.

The method presented in this thesis is more flexible than
the methods based on the ROC rule and can---depending on
the context of the classification problem---produce results
better interpretable than results from a cost setting
(see Chapter~\ref{sec:example}).
Also it is more natural with multi-class classification
problems than the methods based on the ROC rule, all
assuming binary classification problems, wherefore the
classifiers generated by these methods must be transformed
to multi-class classifiers for non-binary problems.

On the other hand the presented method can suffer from its
very general approach and only produces approximations.
This can result in non-optimal and unstable thresholds.

This thesis first presents a motivational example.
In Chapter~\ref{sec:method} the proposed method is
presented.
After that experiments on data sets from the UCI machine
learning repository \citep[see][]{uci} are discussed.
At last further research ideas are listed and a conclusion
is drawn.
% }}}

% Motivational example {{{
\section{Motivational example}
\label{sec:example}

This chapter will point out the usefulness of abstaining
classifiers in real world application domains where
reliability is key.
It will show an example why the reward setting can improve
readability in some domains.
First another example, for which the cost setting---more
commonly used in supervised learning---comes more natural
is given and the differences are discussed.

Abstaining classifiers---compared to typical classifiers,
which classify every prediction, maybe even without a
confidence value in it (then called a bare
prediction)---can be easily integrated into and enhance
processes where they partially replace some of the decision
making, since they can delegate the abstained predictions
back to the underlying process.
The use of abstaining classifiers in domains where
reliability---in regard to prediction errors---is
important, has an interesting aspect in giving
reliability while still being able to decrease work, cost,
etc.\ to some degree.
This is a valuable property if there does not exist a
typical classifier good enough to fully replace the
underlying process.

Many real world application domains for abstaining
classifiers can express a cost function associated to the
decisions about predicting and abstaining of the
classifier---which then chooses the threshold with which it
produces the least amount of cost, therefore minimizing
the cost of introducing the abstaining classifier to the
process.

For example, the real world application domain could be a
facial recognition system at a company which regulates
which employee can enter a trust zone and which can not.
The process which should be enhanced with the facial
recognition system is a manual process where the employee
has to fill out a form in order to receive a key which
opens the trust zone.

In this example, the costs of miss-classifying an
unauthorized person as authorized can be huge for
the company while abstaining or classifying an authorized
employee as unauthorized produces quite low costs---the
authorized employee just has to start the manual process,
which should be replaced by the facial recognition system.

On the other hand, for some real world application domains
a reward function based on which the abstaining classifier
chooses the threshold by maximizing the reward---rather
than minimizing the cost---comes more natural.

Such a domain would be the finance industry,
where we often can associate a certain amount of money an
abstaining classifier can produce or safe by supporting the
decision making of an underlying process.

An example for such a process would be the process of a
bank for granting a consumer credit.
The bank requests information about the consumer from a
credit bureau in order to assess the consumer's credit
default risk.
Now the bank wants to predict the consumer's credit
default risk based on information the bank has about the
consumer.
If the credit default risk is very high or very low the
bank can save money not making a request to the credit
bureau for this consumer.
The optimal threshold for the abstaining classifier making
the prediction about the credit default risk can easily be
expressed by a reward function.
Every correct decision saves the bank the money the request
to the credit bureau costs.
Every miss-classification costs the bank either the amount
of money it would gain by granting the credit, or the
money it loses by giving a credit to somebody that does not
pay the rates.
Abstention cost is the cost of making a request to the
credit bureau.

Using a reward function---like in the example
above---instead of a cost function has an advantage in
readability. One can easily assess the gain of introducing
the abstaining classifier to the process.
Is the reward generated by the abstaining classifier higher
than zero, the process is enhanced by the abstaining
classifier.
Otherwise the abstaining classifier would produce more
cost than it would save and it is not valuable for the
bank to introduce it to its process of assessing a
consumer's credit default risk.
% }}}

% Proposed method based on reward {{{
\section{Proposed method based on reward}
\label{sec:method}

Let $\textbf{X}$ be the observation space and $\textbf{Y}$
the label space. $|\textbf{Y}| < \infty$ since only
classification is discussed. Let $\textbf{Z}$ be the
cartesian product of $\textbf{X}$ and $\textbf{Y}$:
$\textbf{Z} := \textbf{X} \times \textbf{Y}$.
$\textbf{Z}$ is called the example space.
Let an example $z_i$ from $\textbf{Z}$ be:
$z_i := (x_i, y_i); z_i \in \textbf{Z}$.
A data set\footnote{not an actual set but a multi-set since
it can contain the same element more often than one time.}
containing examples $z_1,\dots,z_n$ is annotated as
$\ds$.

% Scoring classifiers {{{
\subsection{Scoring classifiers}

A classical machine learning predictor---in the previous
chapter called a typical classifier---can be represented
by a function
\begin{align}
  \label{eq:D}
  D: \textbf{Z}^* \times \textbf{X} \rightarrow \textbf{Y}.
\end{align}
Its first argument being a data set with an arbitrary
length the classifier is trained on, while the second is an
observation which should be predicted
(mapped to a label from $\textbf{Y}$).

Let $D_{\ds}$ be a classical machine
learning predictor trained on the data set $\ds$ and
let $D_{\ds}(x)$ be equivalent to (\ref{eq:D}) with the
first argument being $\ds$.

The proposed method relies on scoring classifiers.
A scoring classifier does not return just
a label but instead returns some score for each label from
the label space.
The scores can be either ascending or descending in order,
ascending meaning higher scores are better than lower;
descending is the opposite.
A score could be a probability or just an uncalibrated
confidence value \citep[see][]{vanderlooy_et_al_2009}.

Let $S$ be a scoring classifier:
\begin{align}
  \label{eq:S}
  S: \textbf{Z}^* \times \textbf{X} \rightarrow
     (\textbf{Y} \rightarrow \mathbb{R}).
\end{align}
$S$ takes the same arguments as (\ref{eq:D}) but instead
of producing bare predictions it returns a function which
maps every label from the label space to a score
determined by $S$.

The method proposed is only interested in the highest
score and the associated label. For that two functions
$k$ and $v$ are defined:
\begin{align*}
  k(S_{\ds}, x) &= \argmax_{y \in \textbf{Y}}
    S_{\ds}(x)(y) \\
  v(S_{\ds}, x) &= \max_{y \in \textbf{Y}}
    S_{\ds}(x)(y).
\end{align*}

The composition $kv$ of $k$ and $v$ returns the tuple with
the label mapped to the highest score:
\begin{align}
  \label{eq:kv}
  kv(S_{\ds}, x) = (k(S_{\ds}, x), v(S_{\ds}, x)).
\end{align}
% }}}

% Abstaining classifiers {{{
\subsection{Abstaining classifiers}

An abstaining classifier $A$ can be defined as a similar
function as (\ref{eq:D}), with the only difference being
the return value:
\begin{align*}
  A: \textbf{Z} \times \textbf{X} \rightarrow
      \textbf{Y} \cup \{\bot\}
\end{align*}
$A$ can return a label from $\textbf{Y}$, but also $\bot$,
indicating that $A$ would like to abstain from making a
prediction.

Let $\S$ be the set of all scoring classifiers defined like
(\ref{eq:S}) on the example set $\Z$.
The proposed method is interested in transforming a
scoring classifier $S \in \S$ to an abstaining classifier
$A$.
In order to do that a threshold $T \in \mathbb{R}$ is
defined and $A$ can be represented as a composition of $S$
and $T$.
Let $S_{\ds}$ be a scoring classifier which produces scores
in ascending order, $T$ a threshold and $x$ an observation
to be predicted.
The abstaining classifier $A$ composed of $S_{\ds}$ and $T$
predicts $x$ as follows:
\begin{align}
  \label{eq:A}
  A(\ds, x) =
    \begin{cases}
      k(S_{\ds}, x) &\qquad \text{if }v(S_{\ds}, x)\geq T\\
      \bot &\qquad \text{if } v(S_{\ds}, x) < T
    \end{cases}.
\end{align}
The equivalent $A$ if $S_{\ds}$ is a scoring classifier
that produces scores in descending order just exchanges the
comparison operators with their respective opposite:
\begin{align}
  \label{eq:A2}
  A(\ds, x) =
    \begin{cases}
      k(S_{\ds}, x) &\qquad \text{if }v(S_{\ds}, x)\leq T\\
      \bot &\qquad \text{if } v(S_{\ds}, x) > T
    \end{cases}.
\end{align}

This representation of $A$ is rather unconventional and
is one reason the proposed method is unstable.

Using a single threshold for all labels is a strong
constraint to put onto the scoring classifier, because it
must be invariant to the label distribution.
Imagine a classification problem where one label makes up
90 percent of all examples and the scoring classifier is
not invariant to the label distribution.
This could lead the classifier to produce higher scores for
observations with the label which makes up 90 percent.
This could result in an abstaining classifier that does not
predict an any example which does not have the dominant
label, even though with such a distribution predicting
the submissive labels would probably be more interesting.

ROC based and other methods for generating abstaining
classifiers address this problem by using abstention
windows instead of a single threshold
\citep[see][]{friedel_et_al_2006}.

Let $\Y$ be a binary problem $\Y := \{P, N\}$,
where $P$ is called the positive label and $N$ the
negative label.
The margin $m: \Y \times \mathbb{R} \rightarrow (-1,1)$ is
a function that combines the label with the confidence
value and returns a number in the interval of $(-1,1)$.
The closer the return value of $m$ is to the edges of the
interval, the more confident the scoring classifier is,
whereby -1 means perfectly confident the label is $N$ and 1
means perfectly confident the label is $P$
\citep[see][]{friedel_et_al_2006}.

In \citet{guan_et_al_2018} a similar method is described,
constraining the output of the margin $m$ not on $(-1,1)$
but instead using only the likelihood of an observation $x$
having the positive label $P$
($m: \mathbb{R} \rightarrow (0,1)$).

Both \citet{friedel_et_al_2006} and \citet{guan_et_al_2018}
define an abstention window $a$ as a tuple
$a := (t_1, t_2); t_1 < t_2$ with two thresholds.
An abstaining classifier of the form described in
(\ref{eq:A}) with an abstention window instead of a
threshold predicts an observation $x$ as:
\begin{align*}
  A(\ds, x) =
    \begin{cases}
      P    &\qquad \text{if } m(kv(S_{\ds}, x)) > t_2 \\
      \bot &\qquad \text{if }
            t_1 \leq m(kv(S_{\ds}, x)) \leq t_2 \\
      N    &\qquad \text{if } m(kv(S_{\ds}, x)) < t_1 \\
    \end{cases}.
\end{align*}
This addresses the problem of using a single threshold $T$
for predictions on both labels from $\Y$.
The constraint of abstention windows is that they are only
defined on binary problems and must be transformed in order
to use them in a multi-class setting.
This could be done with the one-vs-one or the one-vs-all
approach, in which multiple binary classifiers are learned
\citep[see e.g.][Chapter 14.5]{murphy_2012}.
But, like stated in \citet{friedel_2005} multi-class
problems increase the complexity of ROC based and other
methods, because when using a one-vs-one or one-vs-all
approach it is possible that more than one label gets
predicted by the abstaining classifier
\citep[see][]{friedel_2005}.

On the other hand an arbitrary number of labels can be
predicted with a single threshold, though the solution
could be sub-optimal and is depending heavily on the
underlying scoring classifier.

This thesis does not address the problem of using a single
threshold in the empirical
study presented in Chapter~\ref{sec:experiments}, but a
possible solution is given in
Chapter~\ref{sec:further_research}.
% }}}

% Abstaining classifiers from reward {{{
\subsection{Abstaining classifiers based on a reward
    system}
\label{subsec:reward}

The novel approach of this thesis is using a system based
on reward which is maximized rather then cost that is
minimized in order to determine the optimal threshold for
abstention.
Like stated in Chapter~\ref{sec:intro} using a reward
function---like used in reinforcement learning---in a
supervised learning setting is rather uncommon.
In Chapter~\ref{sec:intro} and Chapter~\ref{sec:example}
some reasons why using reward instead of cost are given.

Another aspect of cost, which makes it less flexible than
reward, not previously discussed, is that it is only
defined on $\mathbb{R}^+$, while reward is defined on
$\mathbb{R}$.
Reward combines cost with gain.

Let $\rho$ be a reward function:
\begin{align}
  \label{eq:rho}
  \rho: \Y^* \times \hat{\Y}^* \rightarrow \mathbb{R}^*.
\end{align}
$\rho$ takes two arbitrary, but equal long vectors with
labels from $\Y$ and from $\hat{\Y}$. $\hat{\Y}$ can be
equal to $\Y$ or also contain an element indicating
abstention $\bot$.
The first vector contains the true labels of some sequence
of examples, the second contains the predicted labels from
some classifier for the same sequence.
$\rho$ returns a reward for each tuple of true
label and predicted label from the parameter vectors.

The reward function is basically treated as a black box
function; the only knowledge we have is, whether $\rho$
produces single-step reward or accumulated reward values
and whether $\rho$ is stateful or stateless
(see Table~\ref{tab:reward_prop}).

\begin{table}
  \begin{center}
  \begin{tabu}{lcc}
    &stateless &stateful \\ \cline{2-3}
    accumulated &\multicolumn{1}{|c|}{true}
                &\multicolumn{1}{c|}{true} \\ \cline{2-3}
    single step &\multicolumn{1}{|c|}{true}
                &\multicolumn{1}{c|}{false} \\ \cline{2-3}
  \end{tabu}
  \end{center}
  \caption{Possible combinations of the two known
           properties of a reward function. It is not
           possible to have a stateful single step reward
           function, because a single step reward function
           is only dependent on the true and predicted
           label of one example.}
  \label{tab:reward_prop}
\end{table}

Treating the reward function this way makes it more
flexible than a cost setting which uses cost matrices
\citep[see][]{fisher_et_al_2016}. A cost matrix $C$ for a
binary abstaining classifier is defined as
\begin{align*}
  C :=
    \begin{pmatrix}
      C(P, P) &C(P, N) &C(P, \bot) \\
      C(N, P) &C(N, N) &C(N, \bot) \\
    \end{pmatrix}.
\end{align*}
A cost function $c$ with the same definition as
(\ref{eq:rho}) based on such a cost matrix $C$ would be
defined as $c(\vec{t}, \vec{p}) = [C(t_i, p_i);
i=1,\dots,|\vec{t}|]^T$ and is basically the inverse of a
single step reward function---with the difference that
$C(P, P)$ and $C(N, N)$ normally do not have a gain
associated to them, because then the cost matrix would not
be true to its cost setting.
A cell of a cost matrix $C$ would provide a gain if its
value is smaller than zero.

A reward function that returns already accumulated rewards
provides an even more flexible setting than single step
reward---which is only dependent on one example's true and
predicted label---because it can introduce the concept of
state \citep[see][Chapter 1]{sutton_et_al_2018}.

For example, the abstaining classifier could be a bettor
betting on the outcome of a card game.
It starts with a certain amount of money and always bets
two thirds of the amount it currently has.
Every example is one match and it is possible to derive a
certainty measure based on some information about the
match.
The reward is the amount of money the classifier wins or
loses.
It gains a certain amount---depending on how much money the
classifier owns after the last match it has bet on---if
it decides to bet on the current match and does so
correctly or loses two thirds of its reward up to the
current bet if the classifier was wrong.
A reward function like this is not stateless like a
single step reward function and is a commonly used
in the reinforcement learning setting
\citep[see][Chapter 1]{sutton_et_al_2018}.

An interesting question is where to draw the line between
an abstaining classifier that maximizes reward and a
reinforcement learning agent, because the bettor described
above could also be defined as a reinforcement learning
agent.
This thesis will not declare a clear differentiation
between the two concepts, but the interaction with the
environment seems to be a good point for differentiation.
If the predictions of the abstaining classifier alter
reality (the predictions of the bettor above most certainly
would change reality) it behaves like an agent, otherwise
it is just an abstaining classifier.

An argument for such a differentiation would be, that
supervised learning---on which the focus of this thesis
lies---is underlined by the assumption, that all
observations $x_i \in \X$ observed are independent from the
other observations $x_j \in \X$, but that they share the
same unknown distribution.
This assumption is called the iid assumption (independent
and identically distributed) \citep[see][]{clauset_2011}
and makes the concept of state irrelevant to our
observations, which would not be the case if the
predictions alter reality.
% }}}

% Method {{{
\subsection{Method for approximating the optimal threshold
  for abstention based on a reward function}

For approximating the optimal threshold---which maximizes
the expected reward in the proposed reward setting---an
architecture comparable to and influenced by the
meta-confor\-mal prediction approach described in
\citet{smirnov_et_al_2009} is proposed.
The architecture of an abstaining classifier based on
reward is comparable to the combined classifier used for
meta-conformal prediction.
A combined classifier \textit{B:M} uses a base classifier
$B$ defined like (\ref{eq:D}) and a conformal predictor $M$
in order to extend $B$ with a confidence measure.
\textit{B:M} can then be transformed to an abstaining
classifier by defining a threshold $T$ in the confidence
values generated by $M$ using the ROC isometrics approach
\citep[see][]{smirnov_et_al_2009, vanderlooy_et_al_2009,
  fassbender_2019}.

An abstaining classifier $A$---defined like (\ref{eq:A}) or
(\ref{eq:A2})---in the reward based
setting described above can also be described as a
combined classifier $A := \text{\textit{S:Reg}}$.
$S$ is a scoring classifier defined like (\ref{eq:S}) and
$Reg$ is a regressor (defined like (\ref{eq:D}) with
$\Y := \mathbb{R}$).
$Reg$ is not necessarily needed and is only used in order
to determine the threshold $T$ for $A$, which can be done
by other means described below.
Chapter~\ref{sec:experiments} shows how well using
different regressors perform in comparison to just taking
the threshold which has generated the highest reward on the
training set.

The threshold of $A$ that approximates the maximum expected
reward is defined during the training phase.
Let $\ds$ be a training set.
$\ds$ is split into $k$ roughly equal sized partitions
using the $k$-fold method \citep[see][Chapter 7.10;
Algorithm~\ref{alg:method}, line 2]{hastie_et_al_2009}.

% algo method {{{
\begin{algorithm}
  \caption{: k-fold method for determining the threshold
             for an abstaining classifier based on a
             reward function}
  \label{alg:method}

  \textbf{Input:}

  \quad $S$: a socring classifier,

  \quad $\rho$: a stateless reward function defined like
        (\ref{eq:rho}),

  \quad data set: $\ds$,

  \quad $k$: the amount of partitions,

  \quad $Reg$: a regressor (optional)

  \textbf{Output:}

  \quad T: threshold

  \begin{algorithmic}[1]
    \STATE{$P := \{\}$}
    \STATE{split data set into $k$ roughly equal sized
           partitions $\text{split}_1,\dots,
           \text{split}_k$}
    \FORALL{$\text{split}_i, i=1,\dots,k$}
      \STATE{combine all $\text{splits} \neq
             \text{split}_i$ to a training set}
      \STATE{train $S$ with the training set}
      \STATE{let $S$ predict examples in $\text{split}_i$}
      \FORALL{elements in prediction of $S$ $\times$
              the true labels of $\text{split}_i$}
        \STATE{get the label associated with the highest
              score for the element with (\ref{eq:kv})}
        \STATE{add the true label, the predicted label and
               the score to $P$}
      \ENDFOR
    \ENDFOR
    \STATE{sort $P$ based on the scores. In ascending order
           if $S$ returns ascending scores. Otherwise in
           descending order}
    \STATE{$R := \text{scores from } P \times \rho(P)$}
    \IF{$\rho$ is a single step reward function}
      \STATE{accumulate reward in $R$}
    \ENDIF
    \STATE{reduce $R$ so all scores are unique
           (optional)}
    \STATE{train $Reg$ (optional)}
    \STATE{determine $T$ with (\ref{eq:T_Reg}) or
           (\ref{eq:T_R})}
    \RETURN{$T$}
  \end{algorithmic}
\end{algorithm}
% }}}

For each partition combine the other $k-1$ partitions to a
training set; train a scoring classifier $S$ on this
set and let it predict on the partition it was not trained
on.
Add the true labels from the examples in the predicted
partition and $kv$ (see Equation~\ref{eq:kv}) of all
predictions to a prediction set
$P \subseteq \Y^n \times \hat{Y}^n \times \mathbb{R}^n$
(see Algorithm~\ref{alg:method}, lines 3--11).

After that $P$ is sorted based on the scores, transforming
it into a sequence.
If $S$ produces scores in ascending order $P$ is also
sorted in ascending order, otherwise in descending order.
This is done in order to simplify reward accumulation,
since the problem of finding the reward at a certain
threshold is reduces to just taking all elements from $P$
which come first in the sequence.
The reward from the first two columns of $P$---with a
reward function $\rho$ defined like (\ref{eq:rho})---is
computed.
Since iid (see previous chapter) is assumed, the parameter
vectors for $\rho$ can be provided with any ordering and
stateful reward functions must also assume iid on the
sequence it sees as its parameters.
Every reward is related to an element from $P$ and the
reward is combined with the scores from $P$ to build the
reward points $R$. $R$ can be represented as a matrix
$R: n \times 2$, where the first column contains all scores
and the second column contains the associated reward.
\begin{align*}
  R :=
    \begin{pmatrix}
      s_1 &r_1 \\
      \vdots &\vdots \\
      s_n &r_n
    \end{pmatrix}
\end{align*}
(see Algorithm~\ref{alg:method}, lines 12, 13).

If $\rho$ is a single step reward function the rewards
are accumulated.
Since $R$ is sorted based on the scores the reward at a
single point is the sum of all rewards previously seen.
The accumulated version of $R$ is
$R^\prime := [(s_i, \sum_{j=1}^{i} r_j); i=1,\dots,n]^T$
(see Algorithm~\ref{alg:method}, lines 14--16).

At last $T$ is derived from $R$. If $Reg$ is defined, it
is trained on $R$, with $s_i$ as observation and $r_i$ as
the label. $T$ is set equal to the score for which $Reg$
predicts the highest reward; the local maximum of $Reg$.
Only the local maximum is of interest, which means $T$
must lie in the interval derived from the convex hull
$C := \text{Conv}(\{i=1,\dots,n:R_{i1}\})$ of
all scores generated from $S$ during the training:
\begin{align}
  \label{eq:T_Reg}
  T := \argmax_{s | \min C \leq s \leq \max C} Reg(s).
\end{align}

% reduce R {{{
\begin{figure}
  \begin{center}
    \begin{tikzpicture}
      \matrix (m0)
      {
        \node{$s_i$}; &\node{$r_i$}; &[2cm]\node{$s_i$};
                                     &\node{$r_i$}; \\
        \node{0.98};  &\node{1}; & &\\
        \node{0.98};  &\node{2}; &\node{0.98}; &\node{2};\\
        \node{0.97};  &\node{3}; & &\\
        \node{0.97};  &\node (x) {2}; & &\\
        \node{0.97};  &\node{1}; &\node{0.97}; &\node{1};\\
        \node{0.96};  &\node{0};&\node{0.96};  &\node{0};\\
      };
      \node[right=0.7 of x]{$\Rightarrow$};
    \end{tikzpicture}
  \end{center}
  \caption{An example for reducing $R$ built from a
           single-step reward function which gives $+1$ for
           a correct prediction and $-1$ for a false one.
           $R$ is already accumulated.
           If $T$ would be determined from the unreduced
           $R$, $0.97$ would be the optimal threshold,
           because it is $\argmax_{r_i} R$. The problem is
           that two errors produced by the scoring
           classifier for examples with the same
           certainty $0.97$ are concealed. Reducing $R$
           to only the last element of each score makes
           certain that no concealing can happen.}
  \label{fig:reduce}
\end{figure}
% }}}

If $Reg$ is not defined $T$ could be derived from $R$
by taking the score which has the highest associated
reward
\begin{align}
  \label{eq:T_R}
  T := R_{i1}: \argmax_{i|1 \leq i \leq n} f(i) = R_{i2}.
\end{align}
For determining $T$ like this, $R$ would be reduced so each
score is unique, since $T$ can only split between two
scores $s_i, s_j$, if $s_i \neq s_j$.
This step is optional, because maybe for use with $Reg$ an
unreduced set of points is wanted
(see Algorithm~\ref{alg:method}, line 17).

Making $R$ unique could be done in different ways, for
example---if $\rho$ is a single step reward function---it
would make sense to take the last tuple of a sub-sequence
where each tuple has the same score, since it contains the
most information about the reward
(see Figure~\ref{fig:reduce}).
One could also reduce them by averaging their rewards, etc.

% }}}

% rl vs ac {{{
\subsection{Equivalences to reinforcement learning}
\label{subsec:rl_vs_ac}

Using a reward system---like described above---to determine
an abstaining classifier makes the whole process quite
similar to the whole setting of reinforcement learning; not
only the reward part. This chapter lists some more aspects
which makes a reinforcement learning agent and an
abstaining classifier look alike, but also shows where
both concepts differ.

A reinforcement learning agent, also called the autonomous
agent, observes---for each (time-)step $t$---a state $s_t$
from its environment.
Based on $s_t$ the autonomous agent takes an action $a_t$
and the environment transitions to a new state $s_{t+1}$.
At each transition the environment provides a reward
value; a feedback for the agent on how well it performs.
The agent learns a policy with which it maximizes the
expected reward \citep[see][]{arulkumaran_et_al_2017}.

An abstaining classifier works quite the same way.
It observes an observation $x_t \in \X$.
For $x_t$ it produces a prediction $p_t \in \Y \cup
\{\bot\}$.

The reward system works a little differently than the one
used in reinforcement learning. Assuming a direct reward
would mean that the abstaining classifier is used in the
perfect online setting in which reality provides the
correct answer after every prediction, which is seldom the
case and would make the abstaining classifier redundant
\citep[see][Chapter 4.3]{alrw}.
While the autonomous agent can use trial and error in order
to increase the success of its policy, the abstaining
classifier is bound to the already observed data and can
only try to generalize from the previous observations to
unseen ones.
The equivalent of the abstaining classifier to the policy
of the agent would be its threshold $T$ (see
Table~\ref{tab:rl_vs_ac}).

The most obvious difference is that the agent actively
interacts with the environment, while the abstaining
classifier should be irrelevant to its
environment---reality providing the classifier with
examples but without assuming the predictions in any case
alter the environment, because it would violate the iid
assumption. The only way interaction with the
environment can be indirectly represented is through a
stateful reward function, which can simulate decisions made
by the abstaining classifier
(see Chapter~\ref{subsec:reward}).

\begin{table}
  \begin{center}
  \begin{tabu}{l|l}
    reinforcement learning agent &abstaining classifier \\
    \hline
    state $s_i$ &observation $x_i$ \\
    action $a_i$ &prediction $p_i$ \\
    environment &reality providing examples from $\Z$ \\
    action changes state of environment
    &iid assumption (reality not altered by predictions)\\
    policy $\pi$ &threshold $T$ \\
    trial and error &examples from reality
  \end{tabu}
  \end{center}
  \caption{Comparison of a reinforcement learning agent
            with an abstaining classifier in the reward
            setting.}
  \label{tab:rl_vs_ac}
\end{table}
% }}}

% }}}

% Experiments {{{
\section{Experiments}
\label{sec:experiments}

This chapter will show some experiments in which the
proposed method, with different configurations, is tested
on real-world data sets from the UCI machine learning
repository \citep[see][]{uci}.

The configuration contains two different scoring
classifiers, eighteen reward functions and 5
regressors---approximating the optimal threshold like
(\ref{eq:T_Reg})---plus the bare threshold derived like
(\ref{eq:T_R}).

% data sets {{{
\subsection{Data sets}

Six data sets where chosen for the experiments. The first
being the bank marketing data set\footnote{\url{%
  https://archive.ics.uci.edu/ml/datasets/bank+marketing}}
(\texttt{bank}).
This data set contains information about the success of
a marketing campaign (phone calls) of a Portuguese bank.
The goal is to predict whether a phone call to a potential
customer results in success, which means the potential
customer subscribes to a term deposit
\citep[see][]{moro_et_al_2014}.
The data set has seventeen features and is a binary
classification problem with 41,188 examples.
The data set is unbalanced, it contains far less successful
phone calls then unsuccessful ones
\citep[see][]{moro_et_al_2014}.

The second data set tested was \texttt{bank-additional}.
It is the same data set as \texttt{bank}, but has three
more features.

The third data set is the car evaluation data set%
\footnote{\url{https://archive.ics.uci.edu/ml/datasets/%
  Car+Evaluation}} (\texttt{car}).
It is described in \citet{bohanec_et_al_1988} and contains
1,728 examples with six attributes. Noteworthy is the fact
that all features and the label are discrete with
just three or four manifestations.

% credit card
Also tested where the default of credit card clients data
set\footnote{\url{https://archive.ics.uci.edu/ml/datasets/%
  default+of+credit+card+clients}} (\texttt{credit card}).
It contains information about default payments in Taiwan
and the goal is to predict whether an observation
represents a credible client or not
\citep[see][]{yeh_et_al_2009}.
This is closely related to the example described in
Chapter~\ref{sec:example}.

Probably the most famous data set used is USPS data set
(\texttt{usps}). It is used in hundreds of papers and
books.
It contains 9,298 examples (images) of handwritten digits
from real life zip codes collected by the US Postal Service
office in Buffalo, NY \citep[see][Appendix B.1]{alrw}.
The observations are a $16 \times 16$ matrix where each
cell is in the interval of (âˆ’1, 1).
Each cell represents the brightness of a pixel. The labels
are the interval 0 to 9
\citep[see][]{lecun_et_al_1989, fassbender_2019}.

The last data set tested was the wine quality data set%
\footnote{\url{https://archive.ics.uci.edu/ml/datasets/%
  Wine+Quality}} (\texttt{wine}).
Each example represents a sample of ``vinho verde'' from
northern Portuguese.
The twelve attributes are physicochemical properties of the
sampled wine which are mapped to a sensory output---the
quality of the wine from zero to ten.
The data set is unbalanced in two ways. It contains only
1,599 red wine samples, but 4,898 white wine samples.
The more important imbalance is the distribution of the
label. Most samples have a quality of five or six
\citep[see][]{cortez_et_al_2009}.

\begin{table}
  \begin{center}
  \begin{tabu}{llll}
    data set &\# examples &\# features &$|\Y|$ \\ \hline
    \texttt{bank} &41,188 &17 &2 \\
    \texttt{bank-additional} &41,188 &20 &2 \\
    \texttt{car} &1,728 &6 &4 \\
    \texttt{credit card} &30,000 &24 &2 \\
    \texttt{usps} &9,298 &256 &10 \\
    \texttt{wine} &6,497 &12 &11 \\
  \end{tabu}
  \end{center}
  \caption{Characteristics of the tested data sets.}
\end{table}
% }}}

% scoring classifiers {{{
\subsection{Scoring classifiers}

Two different underlying scoring classifiers were tested.
The first---\texttt{cp}---is a conformal predictor based
on a $1$-nearest neighbor method as its nonconformity
measure.
A conformal predictor---like its name already
suggests---tries to determine a confidence value by
predicting how an example conforms to previous seen
examples.
If it is used in the online setting and $\Z$ is
exchangeable, its outputs are valid, in the sense that a
conformal predictor $\Gamma^{\epsilon}$ makes errors at
a rate of $\epsilon$ or less.
$\epsilon$---in the original setting of conformal
prediction---is defined beforehand and is called the
significance level. $1-\epsilon$ is the confidence level
\citep[see][]{alrw, fassbender_2019}.

The conformal predictor \texttt{cp} was modified to being a
scoring classifier like (\ref{eq:S}), which produces scores
in descending order.
Refer to \citet{fassbender_2019} for more information on
how to modify a conformal predictor to being a scoring
classifier.

A conformal predictor uses an underlying nonconformity
measure $N:\Z^* \times \Z \rightarrow \mathbb{R}$ to
generate a nonconformity score for an example $z := (x,y)$
\citep[see][]{alrw, fassbender_2019}.
The nonconformity score used in the tests is based on
the $1$-nearest neighbor method and looks like:
\begin{align*}
  N(\ds, z) = \frac{\min_{i=1,\dots,n:y_i=y}d(x_i, x)}
                   {\min_{i=1,\dots,n:y_i\neq y}d(x_i,x)}.
\end{align*}
$d(x_i, x)$ is the euclidean distance in the observation
space between $x_i$ and $x$. It divides the closest
distance to an example with the same label with the
closest distance to an example with another label
\citep[see][]{fassbender_2019}.

The nonconformity score is then transformed to a p-value
in order to see how well the example conforms to the
previous seen examples. Let $\alpha_i, i=1,\dots,n$ be the
nonconformity scores of $\ds$. Let $\alpha$ be the
nonconformity score for $z$. The p-value for $z$ is
computed as
\begin{align*}
  \frac{|\{i=1,\dots,n+1: \alpha_i \geq \alpha\}|}{n + 1}.
\end{align*}
The higher the p-value the more conforms $z$ with the
previous seen examples and the confidence is high that the
prediction is correct.

\texttt{cp} is based on the implementation from the
libconform library \citep[see][]{fassbender_2019}.

The second scoring classifier tested was \texttt{rf},
a scoring classifier based on a random forest with 100
trees.

The score for an observation $x$ is computed as the mean
of the predicted label probabilities of all trees.
A tree predicts the label probabilities as the fraction
of examples from the leaf in which $x$ falls.
For example, let $\Y := \{0,1,2\}$ and the leaf in which
$x$ falls contains ten examples from the training set.
Five examples had label 0, three label 1 and two label 2.
The predicted probabilities of the tree are
$[(0,\frac{1}{2}), (1,\frac{3}{10}), (2,\frac{1}{5})]$.

\texttt{rf} transforms the probability predictions to
complementary probabilities in order to make it a scoring
classifier that produces scores in descending order like
\texttt{cp}.

The scoring classifier is based on the implementation from
the scikit-learn library, version 0.21.2 and uses the
default parameters (except the amount of trees)
\citep[see][]{sklearn_api}.

It should be noted that the performance of the scoring
classifiers are deemed irrelevant for the experiments.
The focus of the experiments lies on approximating the
optimal threshold and not on finding the best scoring
classifier.
% }}}

% reward functions {{{
\subsection{Reward functions}

Eighteen reward functions defined like (\ref{eq:rho}) were
tested.

In order to have comparable results for the asymmetric
reward functions three matrices $M^{50}$, $M^{200}$
and $M^{1000}$ were generated randomly\footnote{%
  the pseudo random number generator from the
  Python standard library was used
  \citep[see][Chapter 9.6]{python}.}
beforehand; each with a different degree of freedom in
range which is denoted by the subscripts.
Each matrix was generated with $11 \times 11$ elements.
Every row and every column is associated to one element
in $\Y$ for each data set.
Every label in $\Y$ is an integer that can index the
matrices.
Every row represents a prediction and every column the
true label.
A binary data set, like \texttt{bank}, would index the
first two rows and the first two columns, while
\texttt{wine} uses every row and every column.
For $M^{50}$, $M^{200}$ and $M^{1000}$ refer to
Appendix~\ref{sec:asym_rew_mat}.

Below all tested reward functions with their definitions
are listed.
All, except the reward functions
which add an abstention cost to abstained predictions,
are single step and therefore stateless reward functions.
The reward functions which add an abstention cost are
stateful and return accumulated reward.
Abstaining cost is easily integrated in a reward function,
since its input vectors are ordered after the score.
Every element is assumed to represent the threshold (which
is why the reward function is considered stateful), each
element that comes after is abstained and a cost for each
is substituted from the reward of the element.
In the tested reward function, abstention cost were set to
1 for each abstained example, resulting in abstention costs
of $|\vec{t}| - i$, $i$ being the index of the element
for which the reward is computed, $\vec{t}$ being the
vector of true labels from the parameters.

The tested reward functions are:

% rew fns {{{
\begin{itemize}

  \item \texttt{simple}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
        i=1,\dots,|\vec{t}|:
        \begin{cases}
          1  &\text{if } t_i = p_i \\
          -1 &\text{if } t_i \neq p_i
        \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{simple scaled 1:5}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
        i=1,\dots,|\vec{t}|:
        \begin{cases}
          1  &\text{if } t_i = p_i \\
          -5 &\text{if } t_i \neq p_i
        \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{simple scaled 1:20}
    \begin{align*}
    \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -20 &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{simple scaled 1:100}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -100 &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{asymmetric loss 50}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -M^{50}_{p_i,t_i} &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{asymmetric loss 200}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -M^{200}_{p_i,t_i} &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{asymmetric loss 1000}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -M^{1000}_{p_i,t_i} &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{random loss}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -RND(0,1) &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{random loss scaled 1:5}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -5*RND(0,1) &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{random loss scaled 1:20}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -20 * RND(0,1) &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{random loss scaled 1:100}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        1  &\text{if } t_i = p_i \\
        -100 * RND(0,1) &\text{if } t_i \neq p_i
      \end{cases}\Bigg]^T
    \end{align*}

  \item \texttt{random}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        RND(0,1) &\text{if } t_i = p_i \\
        -RND(0,1) &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{random scaled 1:5}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        RND(0,1)  &\text{if } t_i = p_i \\
        -5 * RND(0,1) &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{random scaled 1:20}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        RND(0,1)  &\text{if } t_i = p_i \\
        -20 * RND(0,1) &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{random scaled 1:100}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|:
      \begin{cases}
        RND(0,1)  &\text{if } t_i = p_i \\
        -100 * RND(0,1) &\text{if } t_i \neq p_i
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{asymmetric 50 abstain}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|: -(|\vec{t}| - i)+\sum_{j=1}^{i}
      \begin{cases}
        M^{50}_{p_j,t_j}  &\text{if } t_j = p_j \\
        -M^{50}_{p_j,t_j} &\text{if } t_j \neq p_j
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{asymmetric 200 abstain}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|: -(|\vec{t}| - i)+\sum_{j=1}^{i}
      \begin{cases}
        M^{200}_{p_j,t_j}  &\text{if } t_j = p_j \\
        -M^{200}_{p_j,t_j} &\text{if } t_j \neq p_j
      \end{cases} \Bigg]^T
    \end{align*}

  \item \texttt{asymmetric 1000 abstain}
    \begin{align*}
      \rho(\vec{t}, \vec{p}) = \Bigg[
      i=1,\dots,|\vec{t}|: -(|\vec{t}| - i)+\sum_{j=1}^{i}
      \begin{cases}
        M^{1000}_{p_j,t_j}  &\text{if } t_j = p_j \\
        -M^{1000}_{p_j,t_j} &\text{if } t_j \neq p_j
      \end{cases} \Bigg]^T
    \end{align*}

\end{itemize}
% }}}

$RND(0,1)$ is a function returning a pseudo random value
from the uniform distribution constrained on the interval
$(0,1)$

% }}}

% regressors {{{
\subsection{Regressors}
\label{subsec:reg}

Five different kernelized regressors where used in order
to test whether using (\ref{eq:T_Reg}) to determine the
optimal threshold can produce better approximations than
deriving the threshold like (\ref{eq:T_R}).
(\ref{eq:T_R}) is called \texttt{bare} in this section.

Three regressors, \texttt{GP [1e-3, 1]},
\texttt{GP [1e-1, 1]} and \texttt{GP [1, 2]} are based on
a Gaussian process, while \texttt{SVR C1} and
\texttt{SVR C100} are based on support vector regression
\citep[see e.g.][Chapters 14, 15]{murphy_2012}.

All regressors based on a Gaussian process are based on
a RBF kernel defined like
\begin{align}
  \label{eq:rbf}
  k(x, x^\prime) = c * \exp \bigg(
    -\frac{\| x - x^\prime \|}{2l^2} \bigg),
\end{align}
$c$ being a constant optimized during training and $l$ is
called the length scale, also optimized during training.
For each regressor $c$ is bound to the interval
$[0.001, 100]$, while the length scale is bound to the
interval in the regressor's name.
The different intervals are designed to generate regressors
with different bias-variance trade-off.
\texttt{GP [1, 2]}, for example, has a high variance, but
low bias, while \texttt{GP [1e-3, 1]} has a low variance
and a high bias.
$c$ and $l$ are optimized during training with the
L-BFGS-B optimization algorithm from the scipy library
\citep[see][]{zhu_et_al_1997, scipy}.

\texttt{SVR C1} and \texttt{SVR C100} are both
$\epsilon$-support vector machines for regression.
They differ in their penalty parameter $C$.
Both are also based on the RBF kernel:
\begin{align}
  \label{eq:rbf_svm}
  k(x, x^\prime) = \exp \bigg(
    -\frac{\| \psi(x) - \psi(x^\prime) \|}{2l^2} \bigg).
\end{align}
(\ref{eq:rbf_svm}) is defined like (\ref{eq:rbf}),
without the $c$ parameter and $x$, $x^\prime$ replaced by
$\psi(x)$ and $\psi(x^\prime)$.
$\psi$ maps an observation from $\X$ to an enlarged space
\citep[see][Chapter 12.3]{hastie_et_al_2009}.

All regressors are based on the implementations from
the scikit-learn library \citep[see][]{sklearn_api}.

% }}}

\subsection{Results}

The settings of the experiments were all the same.
It was tested how well abstaining classifiers (in this
case defined as (\ref{eq:A2}), since both underlying
scoring classifier produce scores with descending order)
with different thresholds determined by
Algorithm~\ref{alg:method} with different settings
(described above) performed. $k$ was always set to five and
each data set was randomly permuted.
The reward points were all reduced like
Figure~\ref{fig:reduce} displays.
Ten percent of the randomly permuted data set were used as
a test set and the rest as training data.

The performance of the underlying scorer was not tested,
only how close the threshold approximates the optimal
result. Therefore, the results only show how much percent
of the best possible reward on the test set the abstaining
classifier achieves with the determined threshold.

The results of each experiment are shown in
Appendix~\ref{sec:res}.
Appendix~\ref{sec:plots} shows some example plots where the
score is mapped to the reward and the predictions of the
regressors are shown.
The example plots are from tests on the \texttt{usps} data
set with \texttt{cp} as scoring classifier.
In Appendix~\ref{sec:metrics_rew} metrics for each reward
function over every experiment are displayed.

Table~\ref{tab:metrics_overall} shows the average, median
and standard deviation of every regressor and \texttt{bare}
over all data sets.
Table~\ref{tab:metrics_scorer} shows the same information
split by the underlying scoring classifier.

The overall performance of each threshold approximator
(\ref{eq:T_Reg}) and (\ref{eq:T_R}) is quite good,
while (\ref{eq:T_R}) (\texttt{bare}) on average
outperformed the thresholds determined by regressors.
It is also more stable, having a standard deviation
$\sigma$ a lot smaller than the other approximators have.
While this can be an argument not to use (\ref{eq:T_Reg}),
actually \texttt{bare} was outperformed 44 percent of the
time by a regressor with an average of nearly two percent
(0.018) more reward on the test set.

The regressors perform better with \texttt{rf} as scoring
classifier than with \texttt{cp}, while \texttt{bare} is
constant on both (see Table~\ref{tab:metrics_scorer}).

While overall the performance of the regressors was
good, on all reward functions with an abstaining cost,
all regressors somewhat failed utterly, having just around
zero percent of the maximum possible reward on the test
set (see Appendix~\ref{sec:res}).
The regressors based on Gaussian processes also struggled
on the higher scaled \texttt{simple} and
\texttt{random loss} reward functions on the \texttt{bank}
data set.
This is why their average is far lower than their mean
(see Table~\ref{tab:metrics_overall}).
\texttt{bare} also failed utterly for some reward functions
with an abstaining cost, but only on the \texttt{car} data
set.

\input{queries}

These utter failures are the reason why this method can not
be called stable and more research must be performed in
order to acquire more knowledge on how to avoid such faulty
behavior.

% }}}

% further research {{{
\section{Further research}
\label{sec:further_research}

While the experiments in Chapter~\ref{sec:experiments}
substantiate the applicability of the proposed method in
its basic setting, many questions are unanswered.
There must be more empirical studies on more elaborate
reward settings in order to see where the method has its
boundaries and how to improve on its stability.
The presented method is very general and the goal of this
thesis was to present its flexible approach and to discuss
its reward based setting.
The idea was to present this method as a foundation for
more specialized and stable methods, incorporating more
knowledge about the domain and more statistical certainty.
Also, the context of this method must be improved,
adding the performance of the underlying scoring classifier
as another point for increasing stability through knowledge
of the system.

While even for asymmetric and random reward functions the
abstaining classifiers trained in
Chapter~\ref{sec:experiments} performed well
(see Appendix~\ref{sec:res}), using a single threshold
can hopefully be improved by an approach again inspired by
a method from the conformal prediction framework:
Mondrian or conditional conformal predictors (MCPs)
\citep[see][]{alrw, cprml, fassbender_2019}.

\def\K{\textbf{K}}

A MCP partitions $\Z$ into a discrete and finite set
$\K$. An element from $\K$ is called a category and
a function
\begin{align*}
  K: \Z^* \rightarrow \K^*
\end{align*}
is called a taxonomy \citep[see][Chapter 2]{cprml}.

The idea would be to use such a taxonomy to categorize
examples and to define a threshold per category, instead of
one threshold for all examples.
This could lead to finer tuned abstaining classifiers,
hopefully increasing their performance.
If this method could increase stateful reward functions
remains an open question and if using such a taxonomy
increases the performance must be tested first.

Another way to increase the performance could be to use
an ensemble of regressors instead of just one to determine
the best threshold, which also remains to be tested.
In general how to chose a good regressor remains an open
question not answered in this thesis.
Like stated in Chapter~\ref{subsec:reg}, the regressors
tested were just to see how different bias-variance
trade-off settings perform.
No tuning process was used to find a regressor that
performs best, just a few parameters were set before the
tests.

A way to decrease training cost would be to replace the
$k$-fold method with a calibration set.
The underlying scoring classifier would be trained on a
training set once and it would predict on the calibration
set instead of splitting the training set into $k$
partitions, resulting in $k$ times the cost of training
the scoring classifier.
This could be useful for big data sets and for scoring
classifier that have a complex training operation, like
support vector machines or neural nets.
% }}}

% conclusion {{{
\section{Conclusion}

The goal of this thesis was to discuss the general
applicability of a reward based setting in order to define
abstaining classifiers.
This thesis also proposes a very general an flexible
setting for defining such a system and to generate an
abstaining classifier from it.
The result of the generality of the method can be unstable
and non-optimal abstaining classifiers, which is
empirically shown on real-world data sets.
On the other hand, the empirical study shows that despite
unstable results the basic method is already applicable to
some degree, producing in general abstaining classifier
which are close to optimal.

In Chapter~\ref{sec:further_research} unanswered questions
are listed and ways to improve the proposed method are
given.

Hopefully the proposed method can be the general framework
on which more specialized and sophisticated approaches are
based and the reward setting of this thesis---and the
resulting convergence of abstaining classifiers to
reinforcement learning agents described in
Chapter~\ref{subsec:rl_vs_ac}---will be further explored.

\newpage

\renewcommand{\appendixpagename}{}
\begin{appendices}
  \section*{Appendix}

  \section{Metrics per reward function}
  \label{sec:metrics_rew}

  \input{rew_metrics}

  \newpage

  \section{Results of the experiments}
  \label{sec:res}

  \input{results}

  \newpage

  \section{Plots from the \texttt{usps} data set with
    \texttt{cp} as scoring classifier}
  \label{sec:plots}

  \input{plots}

  \newpage

  \section{Asymmetric reward matrices}
  \label{sec:asym_rew_mat}

  \setcounter{MaxMatrixCols}{11}

  \begin{align*}
    M^{50} &:=
      \begin{pmatrix}
        37 &13 &18 &13 &16 &46 &43 &48 & 4 &39 &32 \\
        38 &17 &40 &38 &47 & 0 &31 &30 &44 & 5 &23 \\
        29 &50 &20 &31 &48 &50 &18 &11 &28 &37 &10 \\
        20 &37 &40 &31 &23 &20 &13 &27 &34 &34 &11 \\
        41 &42 &37 &37 &38 &40 &45 &39 &24 & 1 &20 \\
         1 &36 & 9 &40 & 8 & 4 &12 & 6 &10 &49 &50 \\
        10 &38 &21 &34 &36 & 6 &43 & 7 &36 &22 &44 \\
        50 &38 & 6 &18 &15 &39 &13 &23 &43 & 7 & 5 \\
        45 &24 &28 &49 &11 & 5 &22 &31 &11 &34 & 9 \\
        50 &22 &11 &25 & 3 &27 &25 &18 &30 & 1 &12 \\
        14 &44 &37 & 1 &34 &17 &46 &30 &50 & 7 &43 \\
      \end{pmatrix} \\ \\
    M^{200} &:=
      \begin{pmatrix}
         59 &199 & 95 &128 &126 &122 & 29 &104 & 12 &164 &200 \\
         81 & 28 &182 & 40 & 60 &156 & 49 & 64 &199 &180 &127 \\
        195 &177 &200 & 10 &188 &198 &105 & 34 & 48 &175 &  3 \\
        142 & 96 & 33 & 92 & 80 &138 & 27 &199 &122 &150 & 56 \\
        186 & 69 &176 &143 &159 & 83 &132 & 79 & 37 & 26 & 72 \\
        156 &110 & 59 &194 &195 & 94 & 66 &171 &180 &  3 &185 \\
        198 &151 &182 &  3 &156 &155 &184 &187 & 94 & 74 & 44 \\
        156 & 21 &200 & 47 &112 & 10 &112 & 32 & 28 &125 &122 \\
        179 & 44 &147 &106 & 61 & 80 &194 & 81 & 78 &124 & 69 \\
         66 & 23 &150 &102 & 11 & 95 & 38 &150 & 75 &121 &  7 \\
        137 & 36 & 84 &119 &120 & 85 &117 & 47 & 60 & 96 &  9 \\
      \end{pmatrix} \\ \\
    M^{1000} &:=
      \begin{pmatrix}
        306 &977 &476 &230 &744 &481 &708 &701 &598 & 30 &954 \\
        637 &369 &  9 &685 &736 &431 &288 &262 & 95 &394 &213 \\
        503 &678 &878 &279 &196 &100 &178 &791 &636 &398 &634 \\
        346 &667 &  2 &294 &172 &260 &412 &211 &832 &675 &756 \\
        278 &396 &516 &674 &769 & 81 &422 &764 &574 &341 &195 \\
        266 &368 &983 &513 &653 &537 &621 &141 &512 &778 & 95 \\
        363 &716 &409 &366 & 96 &951 &782 &573 &125 &633 &623 \\
        922 &184 &708 &153 &511 &649 &621 & 30 &391 &795 &124 \\
        444 & 59 &123 &684 &794 &223 &296 &875 &506 &948 &104 \\
        432 &123 &693 &742 &541 &294 & 12 &  1 &242 &233 &104 \\
        774 &656 &312 &996 &956 &346 &603 &438 &232 &504 &745 \\
      \end{pmatrix} \\
  \end{align*}

\end{appendices}

\bibliography{thesis.bib}

\newpage
\section*{ErklÃ¤rung}
%\markboth{ErklÃ¤rung}{ErklÃ¤rung}\addcontentsline{toc}{section}{ErklÃ¤rung}
Ich versichere, die von mir vorgelegte Arbeit
selbstst\"andig verfasst zu haben.
Alle Stellen, die w\"ortlich oder sinngem\"a{\ss} aus
ver\"offentlichten oder nicht ver\"offentlichten Arbeiten
anderer oder der Verfasserin/des Verfassers selbst
entnommen sind, habe ich als entnommen kenntlich gemacht.
S\"amtliche Quellen und Hilfsmittel, die ich fÃ¼r die Arbeit
benutzt habe, sind angegeben.
Die Arbeit hat mit gleichem Inhalt bzw. in wesentlichen
Teilen noch keiner anderen Pr\"ufungsbeh\"orde vorgelegen.

~\\
~\\
\noindent
\rule{0.35\textwidth}{0.4pt}
\hspace*{3cm}
\rule{0.45\textwidth}{0.4pt}
\newline
Ort, Datum	\hspace*{6.3cm}	Rechtsverbindliche Unterschrift
\end{document}
