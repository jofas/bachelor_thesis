\documentclass[twoside,11pt]{article}
\usepackage{jmlr2e}
\usepackage{amsmath}
\usepackage[page]{appendix}
\usepackage{xcolor}
\usepackage[marginparsep=30pt]{geometry}
\usepackage{stmaryrd}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{tikz}
\usepackage{tabu}
\usepackage{listings}
\usepackage{fancyref}
\usepackage{relsize}

\usetikzlibrary{%
    arrows,
    arrows.meta,
    decorations,
    backgrounds,
    positioning,
    fit,
    petri,
    shadows,
    datavisualization.formats.functions,
    calc,
    shapes,
    shapes.multipart,
    matrix
}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\def\ds{\Lbag z_1,\dots,z_n \Rbag}
\def\X{\textbf{X}}
\def\Y{\textbf{Y}}
\def\Z{\textbf{Z}}
\def\S{\textbf{S}_{\Z}}

\begin{document}
% titlepage {{{
\begin{titlepage}
  \begin{flushleft}
	\vspace*{-1cm}
	\includegraphics[scale=0.05]{TH.png}\\
	\vspace*{1cm}
\end{flushleft}
\begin{center}
\begin{LARGE}
\textbf{%
	Approximating the optimal threshold for an abstaining
  classifier based on a reward function with regression
}
\end{LARGE}
~\\
~\\
~\\
\textit{{\LARGE B}ACHELOR {\LARGE T}HESIS}
~\\
~\\
~\\
\begin{Large}
\begin{tabu} to \textwidth {Xr}
Jonas Fassbender
&\href{mailto:jonas@fassbender.dev}{jonas@fassbender.dev}\\
&11117674
\end{tabu}
\end{Large}
~\\
~\\
~\\
\begin{large}
In the course of studies

\textit{{\Large C}OMPUTER {\Large S}CIENCE}
~\\
~\\
~\\
For the degree of

\textit{{\Large B}ACHELOR OF {\Large S}CIENCE}
~\\
~\\
~\\
Technical University of Cologne

Faculty of Computer Science and Engineering
~\\
~\\
~\\
\begin{tabular}{rl}
  First supervisor: &Prof. Dr. Heinrich Klocke\\
                    &Technical University of Cologne\\
  &\\
  Second supervisor: &Prof. Dr. Fotios Giannakopoulos\\
                     &Technical University of Cologne
\end{tabular}
~\\
~\\
~\\
Overath, July 2019
\end{large}
\end{center}
\end{titlepage}
% }}}

%\begin{abstract}%
%\end{abstract}

%\begin{keywords}
%\end{keywords}

% Introduction {{{
\section{Introduction}
\label{sec:intro}

An abstaining classifier
\citep[see e.g.][]{vanderlooy_et_al_2009}---also called a
classifier with reject option
\citep[see e.g.][]{fisher_et_al_2016}---is a kind
of confidence predictor.
It can refuse from making a prediction if its confidence in
the prediction is not high enough.
High enough, in this context, means that the confidence is
greater than a certain---hopefully optimal---threshold.
Optimality is dependent on a performance metric set
beforehand.

This thesis introduces a new kind of method for
approximating the optimal threshold based on a reward
function---better known from reinforcement learning than
from the supervised learning setting
\citep[see e.g.][Chapter 1]{sutton_et_al_2018}.
The method treats the reward function as unknown, making it
a very general approach and giving quite the amount of
freedom in designing the reward function.

In supervised learning the concept that is closest to a
reward function is a cost function and many abstract types
of cost in supervised learning are known
\citep[see][]{turney_2000}.

Probably today's most used methods for obtaining the
optimal threshold for reducing the expected cost of an
abstaining classifier are based on the receiver operating
characteristic (ROC) rule
\citep[see][]{tortella_2000,pietraszek_2005,
  vanderlooy_et_al_2009, guan_et_al_2018}.

The method presented in this thesis is more flexible than
the methods based on the ROC rule and can---depending on
the context of the classification problem---produce results
better interpretable than results from a cost setting
(see Chapter~\ref{sec:example}).
Also it is more natural with multi-class classification
problems than the methods based on the ROC rule, all
assuming binary classification problems, wherefore the
classifiers generated by these methods must be transformed
to multi-class classifiers for non-binary problems.

On the other hand the presented method can suffer from its
very general approach and only produces approximations.
This can result in non-optimal and unstable thresholds.

This thesis first presents a motivational example.
In Chapter~\ref{sec:method} the proposed method is
presented.
After that experiments on data sets from the UCI machine
learning repository \citep[see][]{uci} are discussed.
At last further research ideas are listed and a conclusion
is drawn.
% }}}

% Motivational example {{{
\section{Motivational example}
\label{sec:example}

This chapter will point out the usefulness of abstaining
classifiers in real world application domains where
reliability is key.
It will show an example why the reward setting can improve
readability in some domains.
First another example, for which the cost setting---more
commonly used in supervised learning---comes more natural
is given and the differences are discussed.

Abstaining classifiers---compared to typical classifiers,
which classify every prediction, maybe even without a
confidence value in it (then called a bare
prediction)---can be easily integrated into and enhance
processes where they partially replace some of the decision
making, since they can delegate the abstained predictions
back to the underlying process.
The use of abstaining classifiers in domains where
reliability---in regard to prediction errors---is
important, has an interesting aspect in giving
reliability while still being able to decrease work, cost,
etc.\ to some degree.
This is a valuable property if there does not exist a
typical classifier good enough to fully replace the
underlying process.

Many real world application domains for abstaining
classifiers can express a cost function associated to the
decisions about predicting and abstaining of the
classifier---which then chooses the threshold with which it
produces the least amount of cost, therefore minimizing
the cost of introducing the abstaining classifier to the
process.

For example, the real world application domain could be a
facial recognition system at a company which regulates
which employee can enter a trust zone and which can not.
The process which should be enhanced with the facial
recognition system is a manual process where the employee
has to fill out a form in order to receive a key which
opens the trust zone.

In this example, the costs of miss-classifying an
unauthorized person as authorized can be huge for
the company while abstaining or classifying an authorized
employee as unauthorized produces quite low costs---the
authorized employee just has to start the manual process,
which should be replaced by the facial recognition system.

On the other hand, for some real world application domains
a reward function based on which the abstaining classifier
chooses the threshold by maximizing the reward---rather
than minimizing the cost---comes more natural.

Such a domain would be the finance industry,
where we often can associate a certain amount of money an
abstaining classifier can produce or safe by supporting the
decision making of an underlying process.

An example for such a process would be the process of a
bank for granting a consumer credit.
The bank requests information about the consumer from a
credit bureau in order to assess the consumer's credit
default risk.
Now the bank wants to predict the consumer's credit
default risk based on information the bank has about the
consumer.
If the credit default risk is very high or very low the
bank can save money not making a request to the credit
bureau for this consumer.
The optimal threshold for the abstaining classifier making
the prediction about the credit default risk can easily be
expressed by a reward function.
Every correct decision saves the bank the money the request
to the credit bureau costs.
Every miss-classification costs the bank either the amount
of money it would gain by granting the credit, or the
money it loses by giving a credit to somebody that does not
pay the rates.
Abstention cost is the cost of making a request to the
credit bureau.

Using a reward function---like in the example
above---instead of a cost function has an advantage in
readability. One can easily assess the gain of introducing
the abstaining classifier to the process.
Is the reward generated by the abstaining classifier higher
than zero, the process is enhanced by the abstaining
classifier.
Otherwise the abstaining classifier would produce more
cost than it would save and it is not valuable for the
bank to introduce it to its process of assessing a
consumer's credit default risk.
% }}}

% Proposed method based on reward {{{
\section{Proposed method based on reward}
\label{sec:method}

Let $\textbf{X}$ be our observation space and $\textbf{Y}$
our label space. $|\textbf{Y}| < \infty$ since only
classification is discussed. Let $\textbf{Z}$ be the
cartesian product of $\textbf{X}$ and $\textbf{Y}$:
$\textbf{Z} := \textbf{X} \times \textbf{Y}$.
$\textbf{Z}$ is called the example space.
Let an example $z_i$ from $\textbf{Z}$ be:
$z_i := (x_i, y_i); z_i \in \textbf{Z}$.
A data set\footnote{not an actual set but a multi-set since
it can contain the same element more often than one time.}
containing examples $z_1,\dots,z_n$ is annotated as
$\ds$.

\subsection{Scoring classifiers}

A classical machine learning predictor---in the previous
chapter called a typical classifier---can be represented
by a function
\begin{align}
  \label{eq:D}
  D: \textbf{Z}^* \times \textbf{X} \rightarrow \textbf{Y}.
\end{align}
Its first argument being a data set with an arbitrary
length the classifier is trained on, while the second is an
observation which should be predicted
(mapped to a label from $\textbf{Y}$).

Let $D_{\ds}$ be a classical machine
learning predictor trained on the data set $\ds$ and
let $D_{\ds}(x)$ be equivalent to (\ref{eq:D}) with the
first argument being $\ds$.

The proposed method relies on scoring classifiers.
A scoring classifier does not return just
a label but instead returns some score for each label from
our label space.
The only constraint on the scores is that higher scores
are better than lower.
A score could be a probability or just an uncalibrated
confidence value \citep[see][]{vanderlooy_et_al_2009}.

Let $S$ be a scoring classifier:
\begin{align}
  \label{eq:S}
  S: \textbf{Z}^* \times \textbf{X} \rightarrow
     (\textbf{Y} \rightarrow \mathbb{R}).
\end{align}
$S$ takes the same arguments as (\ref{eq:D}) but instead
of producing bare predictions it returns a function which
maps every label from the label space to a score
determined by $S$.

The method proposed is only interested in the highest
score and the associated label. For that two functions
$k$ and $v$ are defined:
\begin{align*}
  k(S_{\ds}, x) &= \argmax_{y \in \textbf{Y}}
    S_{\ds}(x)(y) \\
  v(S_{\ds}, x) &= \max_{y \in \textbf{Y}}
    S_{\ds}(x)(y).
\end{align*}

The composition $kv$ of $k$ and $v$ returns the tuple with
the label mapped to the highest score:
\begin{align}
  \label{eq:kv}
  kv(S_{\ds}, x) = (k(S_{\ds}, x), v(S_{\ds}, x)).
\end{align}

\subsection{Abstaining classifiers}

An abstaining classifier $A$ can be defined as a similar
function as (\ref{eq:D}), with the only difference being
the return value:
\begin{align*}
  A: \textbf{Z} \times \textbf{X} \rightarrow
      \textbf{Y} \cup \{\bot\}
\end{align*}
$A$ can return a label from $\textbf{Y}$, but also $\bot$,
indicating that $A$ would like to abstain from making a
prediction.

Let $\S$ be the set of all scoring classifiers defined like
(\ref{eq:S}) on the example set $\Z$.
The proposed method is interested in transforming a
scoring classifier $S \in \S$ to an abstaining classifier
$A$.
In order to do that a threshold $T \in \mathbb{R}$ is
defined and $A$ can be represented as a composition of $S$
and $T$.
Let $S_\ds$ be a scoring classifier, $T$ a threshold and
$x$ an observation to be predicted.
The abstaining classifier $A$ composed of $S_{\ds}$ and $T$
predicts $x$ as follows:
\begin{align}
  \label{eq:A}
  A(\ds, x) =
    \begin{cases}
      k(S_{\ds}, x) &\qquad \text{if } v(S_{\ds}, x) > T \\
      \bot &\qquad \text{if } v(S_{\ds}, x) \leq T
    \end{cases}.
\end{align}
This representation of $A$ is rather unconventional and
is one reason the proposed method is unstable.

Using a single threshold for all labels is a strong
constraint to put onto the scoring classifier, because it
must be invariant to the label distribution.
Imagine a classification problem where one label makes up
90 percent of all examples and the scoring classifier is
not invariant to the label distribution.
This could lead the classifier to produce higher scores for
observations with the label which makes up 90 percent.
This could result in an abstaining classifier that does not
predict an any example which does not have the dominant
label, even though with such a distribution predicting
the submissive labels would probably be more interesting.

ROC based and other methods for generating abstaining
classifiers address this problem by using abstention
windows instead of a single threshold
\citep[see][]{friedel_et_al_2006}.

Let $\Y$ be a binary problem $\Y := \{P, N\}$,
where $P$ is called the positive label and $N$ the
negative label.
The margin $m: \Y \times \mathbb{R} \rightarrow (-1,1)$ is
a function that combines the label with the confidence
value and returns a number in the interval of $(-1,1)$.
The closer the return value of $m$ is to the edges of the
interval, the more confident the scoring classifier is,
whereby -1 means perfectly confident the label is $N$ and 1
means perfectly confident the label is $P$
\citep[see][]{friedel_et_al_2006}.

In \citet{guan_et_al_2018} a similar method is described,
constraining the output of the margin $m$ not on $(-1,1)$
but instead using only the likelihood of an observation $x$
having the positive label $P$
($m: \mathbb{R} \rightarrow (0,1)$).

Both \citet{friedel_et_al_2006} and \citet{guan_et_al_2018}
define an abstention window $a$ as a tuple
$a := (t_1, t_2); t_1 < t_2$ with two thresholds.
An abstaining classifier of the form described in
(\ref{eq:A}) with an abstention window instead of a
threshold predicts an observation $x$ as:
\begin{align*}
  A(\ds, x) =
    \begin{cases}
      P    &\qquad \text{if } m(kv(S_{\ds}, x)) > t_2 \\
      \bot &\qquad \text{if }
            t_1 \leq m(kv(S_{\ds}, x)) \leq t_2 \\
      N    &\qquad \text{if } m(kv(S_{\ds}, x)) < t_1 \\
    \end{cases}.
\end{align*}
This addresses the problem of using a single threshold $T$
for predictions on both labels from $\Y$.
The constraint of abstention windows is that they are only
defined on binary problems and must be transformed in order
to use them in a multi-class setting.
This could be done with the one-vs-one or the one-vs-all
approach, in which multiple binary classifiers are learned
\citep[see e.g.][Chapter 14.5]{murphy_2012}.
But, like stated in \citet{friedel_2005} multi-class
problems increase the complexity of ROC based and other
methods, because when using a one-vs-one or one-vs-all
approach it is possible that more than one label gets
predicted by the abstaining classifier
\citep[see][]{friedel_2005}.

On the other hand an arbitrary number of labels can be
predicted with a single threshold, though the solution
could be sub-optimal and is depending heavily on the
underlying scoring classifier.

This thesis does not address the problem of using a single
threshold in the empirical
study presented in Chapter~\ref{sec:experiments}, but a
possible solution is given in
Chapter~\ref{sec:further_research}.

\subsection{Abstaining classifiers from reward}

The novel approach of this thesis is using a system based
on reward which is maximized rather then cost that is
minimized in order to determine the optimal threshold for
abstention.
Like stated in Chapter~\ref{sec:intro} using a reward
function---like used in reinforcement learning---in a
supervised learning setting is rather uncommon.
In Chapter~\ref{sec:intro} and Chapter~\ref{sec:example}
some reasons why using reward instead of cost are given.

Another aspect of cost, which makes it less flexible than
reward, not previously discussed, is that it is only
defined on $\mathbb{R}^+$, while reward is defined on
$\mathbb{R}$.
Reward combines cost with gain.

Let $\rho$ be a reward function:
\begin{align}
  \label{eq:rho}
  \rho: \Y^* \times \hat{\Y}^* \rightarrow \mathbb{R}^*.
\end{align}
$\rho$ takes two arbitrary, but equal long vectors with
labels from $\Y$ and from $\hat{\Y}$. $\hat{\Y}$ can be
equal to $\Y$ or also contain an element indicating
abstention $\bot$.
The first vector contains the true labels of some sequence
of examples, the second contains the predicted labels from
some classifier for the same sequence.
$\rho$ returns a reward for each tuple of true
label and predicted label from the parameter vectors.

Reward functions, which also take a third vector with the
associated score as argument are also possible:
\begin{align}
  \label{eq:rho2}
  \rho: \Y^* \times \hat{\Y}^* \times \mathbb{R}^*
        \rightarrow \mathbb{R}^*.
\end{align}
They are obviously only defined for scoring classifiers and
abstaining classifiers based on scoring classifiers.

The reward function is basically treated as a black box
function; the only knowledge we have is, whether $\rho$
produces single-step reward or accumulated reward values
and whether $\rho$ is stateful or stateless.

Treating the reward function this way makes it much more
flexible than a cost setting which uses cost matrices
\citep[see][]{fisher_et_al_2016}. A cost matrix $C$ for a
binary abstaining classifier is defined as
\begin{align*}
  C :=
    \begin{pmatrix}
      C(P, P) &C(P, N) &C(P, \bot) \\
      C(N, P) &C(N, N) &C(N, \bot) \\
    \end{pmatrix}.
\end{align*}
A cost function $c$ with the same definition as
(\ref{eq:rho}) based on such a cost matrix $C$ would be
defined as $c(\vec{t}, \vec{p}) = [C(t_i, p_i);
i=1,\dots,|\vec{t}|]^T$ and is basically the inverse of a
single step reward function---with the difference that
$C(P, P)$ and $C(N, N)$ normally do not have a gain
associated to them, because then the cost matrix would not
be true to its cost setting.
A cell of a cost matrix $C$ would provide a gain if its
value is smaller than zero.

A reward function that returns already accumulated rewards
provides an even more flexible setting than single step
reward---which is only dependent on one example's true and
predicted label---because it can introduce the concept of
state \citep[see][Chapter 1]{sutton_et_al_2018}.

For example, our classifier could be a bettor betting on
the outcome of a card game.
It starts with a certain amount of money and always bets
two thirds of its amount.
Every example is one match and it is possible to derive a
certainty measure based on some information about the
match.
The reward is the amount of money the classifier wins or
loses.
It gains a certain amount---depending on how much money the
classifier owns after the last match it has bet on---if
it decides to bet on the current match and does so
correctly or loses two thirds of its reward up to the
current bet if the classifier was wrong.
A reward function like this is not stateless like a
single step reward function and is a commonly used
in the reinforcement learning setting
\citep[see][Chapter 1]{sutton_et_al_2018}.

The method proposed in this thesis only works for
stateless reward functions.
An alternative approach for approximating the optimal
threshold for reward functions with state based on
Bayesian optimization is described in
Chapter~\ref{sec:further_research}.

\subsection{Method for approximating the optimal threshold
  for abstention based on a stateless reward function}

For approximating the optimal threshold---which maximizes
the expected reward---in a stateless reward setting, an
architecture comparable to and influenced by the
meta-conform\-al prediction approach described in
\citet{smirnov_et_al_2009} is proposed.
The architecture of an abstaining classifier based on
reward is comparable to the combined classifier used for
meta-conformal prediction.
A combined classifier \textit{B:M} uses a base classifier
$B$ defined like (\ref{eq:D}) and a conformal predictor $M$
in order to induce $B$ with a confidence measure.
\textit{B:M} can then be transformed to an abstaining
classifier by defining a threshold $T$ in the confidence
values generated by $M$ using the ROC isometrics approach
\citep[see][]{smirnov_et_al_2009, vanderlooy_et_al_2009,
  fassbender2019}.

The threshold $T$ of an abstaining classifier $A$ that
approximates the maximum expected reward is defined during
the training phase.
Let $\ds$ be a training set.
$\ds$ is split into $k$ roughly equal sized partitions
using the $k$-fold method \citep[see][Chapter 7.10;
Algorithm~\ref{alg:method}, line 2]{hastie_et_al_2009}.

For each partition combine the other $k-1$ partitions to a
training set; train a scoring classifier $S$ on this
set and let it predict on the partition it was not trained
on. Add $kv$ (see Equation~\ref{eq:kv}) of the
predictions and the true labels from the predicted
partition to a prediction set
$P \subseteq \Y^n \times \hat{Y}^n \times \mathbb{R}^n$
(see Algorithm~\ref{alg:method}, lines 3--11).

After that the reward---with a reward function
$\rho$---from the prediction set is computed.
Every reward is related to an element from $P$ and the
reward is combined with the scores from $P$ to build the
reward points $R \subseteq (\mathbb{R}^2)^n$, where the
scores are mapped to their associated rewards
(see Algorithm~\ref{alg:method}, lines 12, 13).

Afterwards $R$ is sorted in descending order based on the
scores, transforming it into a sequence.
If $\rho$ is a single step reward function the rewards
are accumulated, which only means that the reward at a
single point is the sum of all rewards with a score higher
than or equal to itself.
Since $R$ is now an already sorted sequence
$R := [(s_i, r_i); i=1,\dots,n]$ the accumulated version
of $R$ is
$R^\prime := [(s_i, \sum_{j=1}^{i} r_j); i=1,\dots,n]$
(see Algorithm~\ref{alg:method}, lines 15--17).

In order to derive $T$ from $R$ one could simply take the
middle between the score which has the highest associated
reward $h := \max_{s_i} R$ and its direct successor in
$\mathbb{R}$,
$h^\prime := \argmin_{s_i} s_i - h; s_i > h, i=1,\dots,n$.
Then $T$ would be equal to $h - \frac{h-h^\prime}{2}$.

For determining $T$ like this, $R$ would be reduced so each
score is unique, since $T$ can only split between two
scores $s_i, s_j$, if $s_i \neq s_j$.
This step is optionally
(see Algorithm~\ref{alg:method}, line 18).

Making $R$ unique could be done in different ways, for
example---if $\rho$ is a single step reward function---it
would make sense to take the last tuple of a sub-sequence
where each tuple has the same score, since it contains the
most information about the reward.
One could also reduce them by averaging their rewards, etc.

Another approach to just determining the best $T$ would be
to train a regression model on $R$ and find for which
score it produces the maximum reward estimation
(see Algorithm~\ref{alg:method})where it
produces its maxiflexible approach to just taking the score
%is a single step reward function---the rewards are
%accumulated (

\begin{algorithm}
  \caption{: k-fold method for determining the threshold
             for an abstaining classifier based on reward}
  \label{alg:method}

  \textbf{Input:}

  \quad $S$: a socring classifier,

  \quad $\rho$: a reward function,

  \quad data set: $\ds$,

  \quad $k$: the amount of partitions,

  \quad $Reg$: a regressor (optionally)

  \textbf{Output:}

  \quad T: threshold

  \begin{algorithmic}[1]
    \STATE{\texttt{predicted points} := $\{\}$}
    \STATE{split data set into $k$ roughly equal sized
           partitions $\text{split}_1,\dots,
           \text{split}_k$}
    \FORALL{$\text{split}_i, i=1,\dots,k$}
      \STATE{combine all $\text{split} \neq
             \text{split}_i$ to the training set}
      \STATE{train $S$ with the training set}
      \STATE{let $S$ predict examples in $\text{split}_i$}
      \FORALL{elements in prediction of $S$ $\times$ the
              the true labels}
        \STATE{get the label associated with the highest
              score for the element with (\ref{eq:kv})}
        \STATE{add the true label, the predicted label and
               the score to \texttt{predicted points}}
      \ENDFOR
    \ENDFOR
    \STATE{\texttt{rewards} := $\rho$ with the true label
           and the predicted label from \texttt{predicted
           points} as arguments}
    \STATE{\texttt{reward points} := scores from
           \texttt{predicted points} $\times$
           \texttt{rewards}}
    \STATE{sort \texttt{reward points} based on the
           scores in descending order}
    \IF{$\rho$ is a single step reward function}
      \STATE{accumulate reward in \texttt{reward points}}
    \ENDIF
    \STATE{reduce \texttt{reward points} so all scores are
           unique (optionally)}
    \STATE{train $Reg$ (optionally)}
    \STATE{$T := \argmax Reg$ or the middle between the
           score which has the highest reward associated}
    \RETURN{$T$}
  \end{algorithmic}
\end{algorithm}

\subsection{Equivalences to reinforcement learning}

% }}}

\section{Experiments}
\label{sec:experiments}

%\section{Other methods for abstaining}
%\label{sec:abs_methods}

\section{Further research}
\label{sec:further_research}

\section{Conclusion}

\renewcommand{\appendixpagename}{}
\begin{appendices}
  \section*{Appendix}

  \section{Plots}

\end{appendices}

\bibliography{thesis.bib}

\newpage
\section*{Erklärung}
%\markboth{Erklärung}{Erklärung}\addcontentsline{toc}{section}{Erklärung}
Ich versichere, die von mir vorgelegte Arbeit
selbstst\"andig verfasst zu haben.
Alle Stellen, die w\"ortlich oder sinngem\"a{\ss} aus
ver\"offentlichten oder nicht ver\"offentlichten Arbeiten
anderer oder der Verfasserin/des Verfassers selbst
entnommen sind, habe ich als entnommen kenntlich gemacht.
S\"amtliche Quellen und Hilfsmittel, die ich für die Arbeit
benutzt habe, sind angegeben.
Die Arbeit hat mit gleichem Inhalt bzw. in wesentlichen
Teilen noch keiner anderen Pr\"ufungsbeh\"orde vorgelegen.

~\\
~\\
\noindent
\rule{0.35\textwidth}{0.4pt}
\hspace*{3cm}
\rule{0.45\textwidth}{0.4pt}
\newline
Ort, Datum	\hspace*{6.3cm}	Rechtsverbindliche Unterschrift
\end{document}
